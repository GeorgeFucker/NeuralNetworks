{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Import NumPy and Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import load_digits, load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I need this constant to prevent log(0) in CrossEntropy loss\n",
    "eps = 1e-7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Utility functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I implemented 3 different activation functions:\n",
    "<table>\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <th>Name</th>\n",
    "            <th>Plot</th>\n",
    "            <th>Equation</th>\n",
    "            <th>Derivative</th>\n",
    "            <th>Range</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td>Identity</td>\n",
    "            <td><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/9/9e/Activation_identity.svg/240px-Activation_identity.svg.png\"></td>\n",
    "            <td><img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/f690285952308aa49e3c6aac892df31cad6d1b06\"></td>\n",
    "            <td><img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/1f82687d38aa641f513d166b138923a84d7aae86\"></td>\n",
    "            <td><img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/0c8c11c44279888c9e395eeb5f45d121348ae10a\"></td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Rectified linear unit (ReLU)</td>\n",
    "            <td><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Activation_rectified_linear.svg/240px-Activation_rectified_linear.svg.png\"></td>\n",
    "            <td><img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/824a1cc623637e8a5c041a4ac3fc96aa70ed88ff\"></td>\n",
    "            <td><img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/e8723cef7eb5dedf4aa20e174ee281b76a6cbec4\"></td>\n",
    "            <td><img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/8dc2d914c2df66bc0f7893bfb8da36766650fe47\"></td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Logistic (Sigmoid)</td>\n",
    "            <td><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/5/5b/Activation_logistic.svg/240px-Activation_logistic.svg.png\"></td>\n",
    "            <td><img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/36f792c44c0a7069ad01386452569d6e34fe95d7\"></td>\n",
    "            <td><img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/50a861269c68b1f1b973155fa40531d83c54c562\"></td>\n",
    "            <td><img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/c79c6838e423c1ed3c7ea532a56dc9f9dae8290b\"></td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "Each one is implemented using OOP design with two mathods:\n",
    "<ul>\n",
    "    <li>__call__(x) - apply function to input <b>x</b>;</li>\n",
    "    <li>prime(x) - apply derivative of appropriate function to the input <b>x</b>.</li>\n",
    "</ul>\n",
    "\n",
    "Also user can add their own function with appropriate derivative using provided structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Identity:\n",
    "    \"\"\"\n",
    "    Implementation of Identity activation function and its derivative: \n",
    "     - f(x) = x\n",
    "     - f'(x) = 1\n",
    "    \n",
    "    Methods\n",
    "    -------\n",
    "    __call__(x)\n",
    "        Evaluates function value at point x.\n",
    "    prime(x)\n",
    "        Evaluates function's derivative value at point x.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        \"\"\"Function call method\n",
    "        \n",
    "        f(x) = x\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x : np.array\n",
    "            A numpy array to input to function.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        np.array\n",
    "            A value of the function of x.\n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def prime(self, x):\n",
    "        \"\"\"Derivative evalution method\n",
    "        \n",
    "        f'(x) = 1\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x : np.array\n",
    "            A numpy array to input to function's derivative.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        np.array\n",
    "            A value of the function's derivative of x.\n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    \"\"\"\n",
    "    Implementation of ReLU activation function: \n",
    "    - f(x) = max(0, x)\n",
    "    - f'(x) = 1 if x < 0 else 1\n",
    "    \n",
    "    Methods\n",
    "    -------\n",
    "    __call__(x)\n",
    "        Evaluates function value at point x.\n",
    "    prime(x)\n",
    "        Evaluates function's derivative value at point x.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        \"\"\"Function call method\n",
    "        \n",
    "        f(x) = max(0, x)\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x : np.array\n",
    "            A numpy array to input to function.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        np.array\n",
    "            A value of the function of x.\n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def prime(self, x):\n",
    "        \"\"\"Derivative evalution method\n",
    "        \n",
    "        f'(x) = 1 if x < 0 else 1\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x : np.array\n",
    "            A numpy array to input to function's derivative.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        np.array\n",
    "            A value of the function's derivative of x.\n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        return (x >= 0).astype('float')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    \"\"\"\n",
    "    Implementation of Sigmoid activation function: \n",
    "    - f(x) = 1 / (1 + exp(-x))\n",
    "    - f'(x) = f(x)(1 - f(x))\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    x : np.array\n",
    "        Cached value of a sigmoid function.\n",
    "    \n",
    "    Methods\n",
    "    -------\n",
    "    __call__(x)\n",
    "        Evaluates function value at point x.\n",
    "    prime(x)\n",
    "        Evaluates function's derivative value at point x.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        \"\"\"Function call method\n",
    "        \n",
    "        f(x) = 1 / (1 + exp(-x))\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x : np.array\n",
    "            A numpy array to input to function.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        np.array\n",
    "            A value of the function of x.\n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        self.x = 1 / (1 + np.exp(-x))\n",
    "        return self.x\n",
    "    \n",
    "    def prime(self, x):\n",
    "        \"\"\"Derivative evalution method\n",
    "        \n",
    "        f'(x) = f(x)(1 - f(x))\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x : np.array\n",
    "            A numpy array to input to function's derivative.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        np.array\n",
    "            A value of the function's derivative of x.\n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        return self.x * (1 - self.x)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <th>Name</th>\n",
    "            <th>Equation</th>\n",
    "            <th>Derivative</th>\n",
    "            <th>Range</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td>Softmax</td>\n",
    "            <td><img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/6d7500d980c313da83e4117da701bf7c8f1982f5\"></td>\n",
    "            <td><img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/81a8feb8f01aaed053c103113e3b4917f936aef0\"></td>\n",
    "            <td><img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/c79c6838e423c1ed3c7ea532a56dc9f9dae8290b\"></td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "Implementation design is the same as usual activation functions have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    \"\"\"\n",
    "    Implementation of Softmax activation function: \n",
    "    - f_i(x) = exp(x_i) / sum(exp(x_j))\n",
    "    - f_i'(x) = f_i(x)(delta_ij - f_j(x))\n",
    "    \n",
    "    Methods\n",
    "    -------\n",
    "    __call__(x)\n",
    "        Evaluates function value at point x.\n",
    "    prime(x)\n",
    "        Evaluates function's derivative value at point x.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        \"\"\"Function call method\n",
    "        \n",
    "        Function implementation is numerically stable to prevent underflow or overflow.\n",
    "        \n",
    "        f_i(x) = exp(x_i) / sum(exp(x_j))\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x : np.array\n",
    "            A numpy array to input to function.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        np.array\n",
    "            A value of the function of x.\n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        # Stabilization\n",
    "        x = x - np.max(x, axis=1, keepdims=True)\n",
    "        num = np.exp(x)\n",
    "        den = np.sum(num, axis=1, keepdims=True)\n",
    "        \n",
    "        return num / den\n",
    "    \n",
    "    def prime(self, x):\n",
    "        \"\"\"Derivative evalution method\n",
    "        \n",
    "        Derivative returns one because we suppose to have derivative of CrossEntropy\n",
    "        loss to ba calculated with association with softmax function. It is much more\n",
    "        easy to implement and it is more efficient solution.\n",
    "        \n",
    "        f_i'(x) = f_i(x)(delta_ij - f_j(x))        \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x : np.array\n",
    "            A numpy array to input to function's derivative.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "            1.\n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>I have two loss function implemented but it is easy to implement your own and use it.</p>\n",
    "<p>\n",
    "    <ul>\n",
    "        <li>\n",
    "            Regression: <img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/e258221518869aa1c6561bb75b99476c4734108e\"/>\n",
    "        </li>\n",
    "        <li>\n",
    "            Classification: <img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcQediZV_J9oWckQU6SMM1bwIJUF05pYb3QJQQhJ3t3YoFcax5Ve\"/>\n",
    "    </ul>\n",
    "</p>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSE:\n",
    "    \"\"\"\n",
    "    Implementation of Mean Squared Error loss function: \n",
    "    - L(y, y_pred) = 1 / 2n * sum((y_i - y_pred_i)^2)\n",
    "    - L'(y, y_pred) = 1 / n * (y - y_pred)\n",
    "    \n",
    "    Methods\n",
    "    -------\n",
    "    __call__(y, y_pred)\n",
    "        Evaluates loss value.\n",
    "    prime(x)\n",
    "        Evaluates function's derivative value.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __call__(self, y, y_pred):\n",
    "        \"\"\"Function call method\n",
    "        \n",
    "        L(y, y_pred) = 1 / 2n * sum((y_i - y_pred_i)^2)\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        y : np.array\n",
    "            A numpy array of real values.\n",
    "        y_pred : np.array\n",
    "            A numpy array of predicted values\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "            A value of the loss.\n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        return np.mean(np.square(y - y_pred)) / 2\n",
    "    \n",
    "    def prime(self, y, y_pred):\n",
    "        \"\"\"Derivative evalution method\n",
    "        \n",
    "        L'(y, y_pred) = 1 / n * (y - y_pred)\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        y : np.array\n",
    "            A numpy array of real values.\n",
    "        y_pred : np.array\n",
    "            A numpy array of predicted values\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        np.array\n",
    "            A numpy array with appropriate derivatives.\n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        return (y - y_pred) / len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropy:\n",
    "    \"\"\"\n",
    "    Implementation of Mean Squared Error loss function: \n",
    "    - L(y, y_pred) = -sum(y_i * log(y_pred_i))\n",
    "    - L'(y, y_pred) = y_pred - y\n",
    "    \n",
    "    Methods\n",
    "    -------\n",
    "    __call__(y, y_pred)\n",
    "        Evaluates loss value.\n",
    "    prime(x)\n",
    "        Evaluates function's derivative value.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __call__(self, y, y_pred):\n",
    "        \"\"\"Function call method\n",
    "        \n",
    "        L(y, y_pred) = -sum(y_i * log(y_pred_i))\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        y : np.array\n",
    "            A numpy array of real values.\n",
    "        y_pred : np.array\n",
    "            A numpy array of predicted values\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "            A value of the loss.\n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        return -np.mean(y * np.log(y_pred + eps))\n",
    "\n",
    "    def prime(self, y, y_pred):\n",
    "        \"\"\"Derivative evalution method\n",
    "        \n",
    "        Given derivative is calculated with dependence on softmax function.\n",
    "        L'(y, y_pred) = y_pred - y\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        y : np.array\n",
    "            A numpy array of real values.\n",
    "        y_pred : np.array\n",
    "            A numpy array of predicted values\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        np.array\n",
    "            A numpy array with appropriate derivatives.\n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        return y_pred - y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I created only one metric: Accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y, y_pred):\n",
    "    \"\"\"Accuracy evalution function\n",
    "        \n",
    "    Parameters\n",
    "    ----------\n",
    "    y : np.array\n",
    "        A numpy array of real values.\n",
    "    y_pred : np.array\n",
    "        A numpy array of predicted value.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        An accuracy of the prediction.\n",
    "            \n",
    "    \"\"\"\n",
    "    \n",
    "    y = np.argmax(y, axis=1)\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "    return (y == y_pred).astype('int').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Other utility functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I defined function which transforms vector of target values to one-hot matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(y):\n",
    "    \"\"\"Function for one-hot transformation of the target values\n",
    "        \n",
    "        This function transformates input vector of target values to one-hot encoded values.\n",
    "        This transformation is neccessary for our Neural Network implementation.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        y : np.array\n",
    "            A numpy array of target values.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        np.array\n",
    "            A numpy array with one-hot encoded values.\n",
    "            \n",
    "        \"\"\"\n",
    "    \n",
    "    y_one_hot = []\n",
    "    maximum = max(y)\n",
    "    for value in y:\n",
    "        row = [1 if i == value else 0 for i in range(maximum + 1)]\n",
    "        y_one_hot.append(row)\n",
    "    return np.array(y_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unison_shuffled_copies(x, y):\n",
    "    \"\"\"Randomly shuffles the data\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : np.array\n",
    "        Input x data.\n",
    "    y : np.array\n",
    "        Input y data.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        A tuple of shuffled x and y arrays.\n",
    "    \n",
    "    \"\"\"\n",
    "    p = np.random.permutation(len(x))\n",
    "    return x[p], y[p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(x, y, size=0.7):\n",
    "    \"\"\"Returns train/test splt data\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : np.array\n",
    "        Input x data.\n",
    "    y : np.array\n",
    "        Input y data\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        Contains x_train, y_train, x_test, y_test\n",
    "    \"\"\"\n",
    "    train_num = np.ceil(len(x) * 0.7).astype(int)\n",
    "    idx = np.random.permutation(len(x))\n",
    "    x, y = x[idx], y[idx]\n",
    "    \n",
    "    return x[:train_num], y[:train_num], x[train_num:], y[train_num:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Structures for Neural Network Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Our basic data structure for neural network is double-linked list. The implementation utilizes a notion of teminated node which points to the end of a list. That node is connected to the first node and last one.</p>\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/5/5e/Doubly-linked-list.svg/1220px-Doubly-linked-list.svg.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Node represents single layer of a neural network. \n",
    "It has connections to previous layer and next layer.\n",
    "Also it has key value to represent a layer if True and nil point if we have termination node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    \"\"\"\n",
    "    Double-linked list node implementation.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    key : bool\n",
    "        Flag of termination node. False if it is termantion one.\n",
    "    prev : Node\n",
    "        Reference to the previous node.\n",
    "    next : Node\n",
    "        Reference to the next node\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, key=True, prev=None, next_=None):\n",
    "        self.key = key\n",
    "        self.prev = prev\n",
    "        self.next = next_\n",
    "        \n",
    "    def __str__(self):\n",
    "        return str(self.key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Double-Linked List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data structure is a basic for our sequential neural network model. It is done only with basic functionality which is necessary for our network model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinkedList:\n",
    "    \"\"\"\n",
    "    Double-linked list implementation.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    nil : Node\n",
    "        Termination node which begins and ends our list.\n",
    "        \n",
    "    Methods\n",
    "    -------\n",
    "    insert(node)\n",
    "        Inserts a node to the end of a list.\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._nil = Node(False)\n",
    "        self._nil.prev = self._nil\n",
    "        self._nil.next = self._nil\n",
    "        \n",
    "    def __str__(self):\n",
    "        string = ''\n",
    "        x = self._nil.next\n",
    "        while x.key:\n",
    "            string += str(x) + ' -> '\n",
    "            x = x.next\n",
    "        return string[:-3]\n",
    "    \n",
    "    def insert(self, node):\n",
    "        \"\"\"Insert a node to the end of a list.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        node : Node\n",
    "            Node to be inserted.\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        node.prev = self._nil.prev\n",
    "        node.next = self._nil\n",
    "        self._nil.prev.next = node\n",
    "        self._nil.prev = node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Layer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(Node):\n",
    "    \"\"\"\n",
    "    This is a basic class. Other layers are inherited from it.\n",
    "    It defines basic functionality that has to be implemented.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    name : str\n",
    "        Layer type name.\n",
    "    shape: tuple\n",
    "        A shape of the layer.\n",
    "        \n",
    "    Methods\n",
    "    -------\n",
    "        build(*args, **kwargs):\n",
    "            Infers layer's weights shape.\n",
    "        \n",
    "        forward(*args, **kwargs)\n",
    "            Does forward propagation through the layer.\n",
    "            \n",
    "        backward(*args, **kwargs)\n",
    "            Does backward propagation through the layer.\n",
    "            \n",
    "        update_weights(*args, **kwargs)\n",
    "            Updates all weights of the class\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self._name = self.__class__.__name__  \n",
    "        self.shape = None\n",
    "    def __str__(self):\n",
    "        return self._name\n",
    "    \n",
    "    def build(self, *args, **kwargs):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, *args, **kwargs):\n",
    "        pass\n",
    "    \n",
    "    def backward(self, *args, **kwargs):\n",
    "        pass\n",
    "    \n",
    "    def update_weights(self, *args, **kwargs):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Layer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Input(Layer):\n",
    "    \"\"\"\n",
    "    This class defines input layer where input data is provided.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "        x : np.array\n",
    "            A numpy array of training data.\n",
    "            \n",
    "    Methods\n",
    "    -------\n",
    "        build(x) \n",
    "            Initializes training data and infer layer's shape\n",
    "        \n",
    "        forward(x)\n",
    "            Does forward propagation (just return input data).\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.x = None\n",
    "        \n",
    "    def build(self, x):\n",
    "        \"\"\"Infer shape of the layer and initialize input data.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "            x : np.array\n",
    "                An array of input data.\n",
    "        \n",
    "        \"\"\"\n",
    "        self.x = x\n",
    "        self.shape = self.x.shape\n",
    "        \n",
    "    def forward(self, *args, **kwargs):\n",
    "        \"\"\"Does forward propagation to the next layer.\n",
    "        \n",
    "        Input layer does not have any weights and biases so we just propagate our input data to the next layer.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        np.array\n",
    "            Input data.\n",
    "        \"\"\"\n",
    "        \n",
    "        return self.x\n",
    "    \n",
    "    @property\n",
    "    def A(self):\n",
    "        \"\"\"This property is defined to be consistent with other layers.\"\"\"\n",
    "        \n",
    "        return self.x\n",
    "    \n",
    "    @property\n",
    "    def weights(self):\n",
    "        \"\"\"This property is defined to be consistent with other layers.\"\"\"\n",
    "        \n",
    "        return self.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(Layer):\n",
    "    \"\"\"\n",
    "    This class defines basic dense layer.\n",
    "    \n",
    "    Dense layer is a layer which dot product and add biass to the input. \n",
    "    After that it uses activation function and propagates this value to the next layer.\n",
    "    \n",
    "    Layer structure:\n",
    "        If we have:\n",
    "            X - input.\n",
    "            W - weights.\n",
    "            b - biases.\n",
    "            f(x) - actiovation function\n",
    "        Then:\n",
    "            Z = dot(X, W) + b\n",
    "            A = f(Z)\n",
    "            \n",
    "    Attributes\n",
    "    ----------\n",
    "    weights : np.array\n",
    "        The matrix of weights.\n",
    "    bias : np.array\n",
    "        The vector of biases.\n",
    "    Z : np.array\n",
    "        Cached value of linear transformation.\n",
    "    A : np.array\n",
    "        Cached value after activation.\n",
    "    dW : np.array\n",
    "        Cached value of loss' derivative w.r.t. weights.\n",
    "    db : np.array\n",
    "        Cached value of loss' derivative w.r.t. bias.\n",
    "        \n",
    "    Methods\n",
    "    -------\n",
    "    init(units, activation=None)\n",
    "        Initialize layer with given number of units and activation function.\n",
    "    build():\n",
    "        Infers layer's weights shape.\n",
    "        \n",
    "    forward(*args, **kwargs)\n",
    "        Does forward propagation through the layer.\n",
    "            \n",
    "    backward(*args, **kwargs)\n",
    "        Does backward propagation through the layer.\n",
    "            \n",
    "    update_weights(*args, **kwargs)\n",
    "        Updates all weights of the class\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, units, activation=None):\n",
    "        \"\"\"Initialization method\n",
    "        \n",
    "        Initializes number of units and activation function in the layer.\n",
    "        \n",
    "        Activation functions available for string parameter:\n",
    "        - 'softmax';\n",
    "        - 'relu';\n",
    "        - 'sigmoid';\n",
    "        - 'identity'.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        units : int\n",
    "            The number of units inside the layer.\n",
    "        activation : class or str\n",
    "            The activation function class which implements function itself and its derivative.\n",
    "        \n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self._units = units\n",
    "        \n",
    "        \n",
    "        if isinstance(activation, str) and activation.lower() == 'softmax':\n",
    "            self._activation = Softmax()\n",
    "        elif isinstance(activation, str) and activation.lower() == 'relu':\n",
    "            self._activation = ReLU()\n",
    "        elif isinstance(activation, str) and activation.lower() == 'sigmoid':\n",
    "            self._activation = Sigmoid()\n",
    "        elif isinstance(activation, str) and activation.lower() == 'identity':\n",
    "            self._activation = Identity()\n",
    "        else:\n",
    "            self._activation = activation()\n",
    "            \n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.Z = None\n",
    "        self.A = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        \n",
    "    def __str__(self):\n",
    "        return super().__str__() + '(' + str(self._units) + ')'\n",
    "    \n",
    "    def build(self):\n",
    "        \"\"\"Infers layer's weights and bias shapes\n",
    "        \n",
    "        At first it infer shape of the layer. \n",
    "        Then weights and bias are initialized randomly with normal distribution.\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        self.shape = self.prev.shape[1], self._units\n",
    "        self.weights = np.random.randn(*self.shape)\n",
    "        self.bias = np.random.randn(self._units)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Propagates activated data to the next layer\n",
    "        \n",
    "        Computes output of the layer:\n",
    "        X - input data.\n",
    "        W - weights.\n",
    "        b - bias.\n",
    "        f(x) - activation function\n",
    "        \n",
    "        Z = dot(X, W) + b\n",
    "        A = f(Z)\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x : np.array\n",
    "            Input data.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        np.array\n",
    "            Propagated through layer data.\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        self.Z = np.dot(x, self.weights) + self.bias\n",
    "        self.A = self._activation(self.Z)\n",
    "\n",
    "        return self.A\n",
    "\n",
    "    def backward(self, dPred):\n",
    "        \"\"\"Calculates gradients of the layer and continue backward propagation further.\n",
    "        \n",
    "        Computes gradients using backprop:\n",
    "        We have:\n",
    "            dA(l) - gradient w.r.t. activation of the current layer. It is defined as input because of neural network design.\n",
    "            Z(l) - cached data after linear transformation of the current layer.\n",
    "            A(l-1) - cached data after activation of the previous layer.\n",
    "            W(l) - weights of the current layer.\n",
    "            f(x) - activation function.\n",
    "        Then:\n",
    "            dZ(l) = dA(l) * f'(Z(l)) - gradient w.r.t. linear transformation of the current layer.\n",
    "            dW(l) = dot(A(l-1).T, dZ(l)) - gradient w.r.t. weights of the current layer.\n",
    "            db(l) = sum(dZ(l)_i) - gradient w.r.t. bias of the current layer.\n",
    "            dA(l-1) = dot(dZ, W(l).T) - gradient w.r.t. activation unit og the previous layer.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        dPred : np.array\n",
    "            Input gradient. In this implementation it is gradient w.r.t. activation unit of the current layer.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        np.array\n",
    "            Output gradient. In this implementation it is gradient w.r.t. activation unit of the previous layer.\n",
    "            \n",
    "        \"\"\"\n",
    "        dZ = np.multiply(dPred, self._activation.prime(self.Z))\n",
    "        self.dW = np.dot(self.prev.A.T, dZ)  \n",
    "        self.db = np.sum(dZ, axis=0, keepdims=True)\n",
    "        dA = np.dot(dZ, self.weights.T)\n",
    "        \n",
    "        return dA\n",
    "    \n",
    "    def update_weights(self, alpha):\n",
    "        \"\"\" Does weight updating.\n",
    "        \n",
    "        It updates weights using gradient descent rule:\n",
    "        W = W - alpha * dW\n",
    "        b = b - alpha * db\n",
    "        \n",
    "        Parameters:\n",
    "            alpha : float\n",
    "                Step-size parameter.\n",
    " \n",
    "        \"\"\"\n",
    "        self.weights = self.weights - alpha * self.dW\n",
    "        self.bias = self.bias - alpha * self.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential(LinkedList):\n",
    "    \"\"\"\n",
    "    Sequential Neural Network model implementation.\n",
    "    \n",
    "    This implementation of neural network gives basic functionality \n",
    "    of adding layers with specific numbers of nodes and build neural network without any problems.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    is_built : bool\n",
    "        Flag that indicates whether model has already been built.\n",
    "    loss : Obj\n",
    "        Loss function with its derivative.\n",
    "    alpha : float\n",
    "        Step-size parameter.\n",
    "    y : np.array\n",
    "        Real target values.\n",
    "    metric : func\n",
    "        Metric which is calculated after training.\n",
    "    \n",
    "    Methods\n",
    "    -------\n",
    "    add(layer)\n",
    "        Adds a layer to the end of the model.\n",
    "    compile(loss, alpha=None, metric=None)\n",
    "        Compile a model with specific loss, step-size and metric.\n",
    "    evaluate(y, y_pred)\n",
    "        Evaluate loss function and metric values with give data.\n",
    "    fit(x, y, epochs=1, shuffle=True)\n",
    "        Build and train neural network.\n",
    "    predict(x)\n",
    "        Makes prediction for the input.\n",
    "    show_shape()\n",
    "        Display shapes of layers.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self._is_built = False\n",
    "        self._loss = None\n",
    "        self.alpha = None\n",
    "        self.y = None\n",
    "        self.metric = None\n",
    "    \n",
    "    def add(self, layer):\n",
    "        \"\"\"Add layer to the end of the model.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : Layer\n",
    "            A layer to be added.\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        self.insert(layer)\n",
    "\n",
    "    def compile(self, loss, alpha=1e-4, metric=accuracy):\n",
    "        \"\"\"Compile model with given loss function, step-size and metric function.\n",
    "        \n",
    "        Provided loss functions:\n",
    "        - 'mse' for Mean Squared Error (regression);\n",
    "        - 'crossentropy' for Cross Entropy loss (classification).\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        loss : Obj or str\n",
    "            Loss function object with function and its derivative evaluation implemented.\n",
    "        alpha : float\n",
    "            Step-size parameter.\n",
    "        metric : func\n",
    "            Metric function.\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        self.alpha = alpha\n",
    "        self.metric = metric\n",
    "        \n",
    "        if isinstance(loss, str) and (loss.lower() == 'mse'):\n",
    "            self._loss = MSE()\n",
    "        elif isinstance(loss, str) and  (loss.lower() == 'crossentropy'):\n",
    "            self._loss = CrossEntropy()\n",
    "        else:\n",
    "            self._loss = loss\n",
    "\n",
    "        # TODO: several metrics handling\n",
    "            \n",
    "    def evaluate(self, x, y):\n",
    "        \"\"\"Evaluates loss function and metric values.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x : np.array\n",
    "            Input data x values.\n",
    "        y : np.array\n",
    "            Real target values.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        tuple\n",
    "            Tuple contains loss function value an metric value\n",
    "        \n",
    "        \"\"\"\n",
    "        y = one_hot(y)\n",
    "        layer = self._nil.next.next\n",
    "        while layer.key:\n",
    "            x = layer.forward(x)\n",
    "            layer = layer.next\n",
    "        y_pred = x\n",
    "        return self._loss(y, y_pred), self.metric(y, y_pred)\n",
    "    \n",
    "    def fit(self, x, y, epochs=1, shuffle=True):\n",
    "        \"\"\"Builds and trains model.\n",
    "        \n",
    "        If you have trained your model already you can continue using this method.\n",
    "        It automatically defines that your model should continue learning because of special flag.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x : np.array\n",
    "            Input data array.\n",
    "        y : np.array\n",
    "            Real target values.\n",
    "        epochs : int\n",
    "            Number of epochs to train.\n",
    "        shuffle : bool\n",
    "            Whether to shuffle input data.\n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        # TODO: validation split implementation\n",
    "        # TODO: validation data implementation\n",
    "        # TODO: visualization of training process\n",
    "        \n",
    "        # Shuffle data if needed\n",
    "        if shuffle:\n",
    "            x, y = unison_shuffled_copies(x, y)\n",
    "        \n",
    "        # Build model if is hasn't been built already\n",
    "        if not self._is_built:\n",
    "            self._build(x, y)\n",
    "        \n",
    "        # Train model\n",
    "        for epoch in range(epochs):\n",
    "            # Forward pass\n",
    "            output = self._forward()\n",
    "            # Calculate loss value and its derivative\n",
    "            loss_value = self._loss(self.y, output)\n",
    "            derivative = self._loss.prime(self.y, output)\n",
    "            # Backward pass\n",
    "            self._backward(derivative)\n",
    "            # Weight updating\n",
    "            self._update_weights()\n",
    "        print('Loss: ', loss_value)\n",
    "        print('Accuracy: ', self.metric(self.y, output))\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"Predicts output with given input.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x : np.array\n",
    "            Input data.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        np.array\n",
    "            Array of predictions.\n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        layer = self._nil.next.next\n",
    "        while layer.key:\n",
    "            x = layer.forward(x)\n",
    "            layer = layer.next\n",
    "        return np.argmax(x)\n",
    "        \n",
    "    def _build(self, x, y):\n",
    "        \"\"\"Build layers and do one-hot encoding of input target variable.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x : np.array\n",
    "            Input data.\n",
    "        y : np.array\n",
    "            Input target variable values.\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        self.y = one_hot(y)\n",
    "        \n",
    "        input_layer = self._nil.next\n",
    "        input_layer.build(x)\n",
    "        \n",
    "        layer = input_layer.next\n",
    "        while layer.key:\n",
    "            layer.build()\n",
    "            layer = layer.next\n",
    "            \n",
    "        self._is_built = True\n",
    "     \n",
    "    def _forward(self):\n",
    "        \"\"\"Does a forward pass layer by layer\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        np.array\n",
    "            Returns output of activation of the last layer.\n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        x = None\n",
    "        layer = self._nil.next\n",
    "        while layer.key:\n",
    "            x = layer.forward(x)\n",
    "            layer = layer.next\n",
    "        return x\n",
    "        \n",
    "    def _backward(self, x):\n",
    "        \"\"\"Does a backward pass layer by layer\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x : np.array\n",
    "            Derivative of the loss function w.r.t. last activation unit.\n",
    "        \n",
    "        \"\"\"\n",
    "        layer = self._nil.prev\n",
    "        while layer.prev.key:\n",
    "            x = layer.backward(x)\n",
    "            layer = layer.prev\n",
    "            \n",
    "    def _update_weights(self):\n",
    "        \"\"\"Updates weights and bias.\"\"\"\n",
    "        layer = self._nil.next\n",
    "        while layer.key:\n",
    "            layer.update_weights(self.alpha)\n",
    "            layer = layer.next\n",
    "\n",
    "    def show_shape(self):\n",
    "        \"\"\"Show model structure.\"\"\"\n",
    "        string = ''\n",
    "        layer = self._nil.next\n",
    "        while layer.key:\n",
    "            string += str(layer.shape) + ' -> '\n",
    "            layer = layer.next\n",
    "        print(string[:-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = load_digits()\n",
    "\n",
    "x, y = data['data'], data['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "x_train, y_train, x_test, y_test = train_test_split(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.0006639189131890422\n",
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "# Build model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Input())\n",
    "model.add(Dense(2048, 'sigmoid'))\n",
    "model.add(Dense(10, 'softmax'))\n",
    "\n",
    "# Compile and train\n",
    "model.compile(loss='crossentropy', alpha=0.0001)\n",
    "model.fit(x_train, y_train, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate loss and accuracy on test data\n",
    "loss, accuracy = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.28188711331426497\n",
      "Accuracy: 0.6827458256029685\n"
     ]
    }
   ],
   "source": [
    "print('Loss:', loss)\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real value: 7\n",
      "Predicted value: 7\n"
     ]
    }
   ],
   "source": [
    "# Show random prediction\n",
    "idx = np.random.randint(len(x))\n",
    "print('Real value:', y[idx])\n",
    "print('Predicted value:', model.predict(x[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
